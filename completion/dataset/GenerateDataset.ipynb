{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "all_nouns = list(wn.all_synsets('n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "word_embedding_matrix = np.load('/u/vendrov/qanda/glove/glove_matrix.npy')\n",
    "word2index = pickle.load(open('/u/vendrov/qanda/glove/glove_word_index.pkl', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "all_words = []\n",
    "all_w2i = OrderedDict()\n",
    "entity_lengths = [] # store length, in words, for each entity\n",
    "entity2word = [] # store indices of words in all entities, in order\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(all_nouns)):\n",
    "    synset = all_nouns[i]\n",
    "    # the embedding of the synset is the embedding of the first lemma\n",
    "    words = []\n",
    "    for lemma in synset.lemma_names():\n",
    "        words = filter(lambda w: len(w) > 0 and w != 's', re.split('[\\W|_]+', lemma))\n",
    "        if words: break;\n",
    "    assert(words)\n",
    "    entity_lengths.append(len(words))\n",
    "    for word in words:\n",
    "        if word not in all_w2i:\n",
    "            all_w2i[word] = len(all_words)\n",
    "            all_words.append(word)\n",
    "        entity2word.append(all_w2i[word])\n",
    "        \n",
    "        \n",
    "in_w2v = np.zeros((len(all_words),))\n",
    "word_embeddings = np.zeros((len(all_words), word_embedding_matrix.shape[1]))\n",
    "for i in range(len(all_words)):\n",
    "    word = all_words[i]\n",
    "    if word in word2index:\n",
    "        word_embeddings[i] = word_embedding_matrix[word2index[word]]\n",
    "        in_w2v[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### get hypernyms\n",
    "\n",
    "# get mapping of synset id to index\n",
    "id2index = {}\n",
    "for i in range(len(all_nouns)):\n",
    "    id2index[all_nouns[i].name()] = i\n",
    "    \n",
    "# get hypernym relations\n",
    "hypernyms = []\n",
    "for synset in all_nouns:\n",
    "    for h in synset.hypernyms() + synset.instance_hypernyms():\n",
    "        hypernyms.append([id2index[synset.name()], id2index[h.name()]])\n",
    "hypernyms = np.array(hypernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nouns[0].substance_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get holonyms\n",
    "holonyms = []\n",
    "for synset in all_nouns:\n",
    "    for h in synset.substance_holonyms() + synset.member_holonyms() + synset.part_holonyms():\n",
    "        holonyms.append([id2index[synset.name()], id2index[h.name()]])\n",
    "holonyms = np.array(holonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22196"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slices = []\n",
    "offset = 0\n",
    "for length in entity_lengths:\n",
    "    slices.append((offset, offset+length))\n",
    "    offset += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 1, 1, 1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('wordnet.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"entity2word\": shape (116007,), type \"<i8\">"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.create_dataset('embeddings', data=word_embeddings)\n",
    "f.create_dataset('in_w2v', data=in_w2v)\n",
    "f.create_dataset('hypernyms', data = hypernyms)\n",
    "f.create_dataset('holonyms', data = holonyms)\n",
    "f.create_dataset('slices', data = np.array(slices))\n",
    "f.create_dataset('entity2word', data = np.array(entity2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save list of synset names\n",
    "names = map(lambda s: s.name(), all_nouns)\n",
    "import json\n",
    "json.dump(names, open('synset_names.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.4585522095627734"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(word_embeddings[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
